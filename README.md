# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparamters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)


## Summary

The dataset contains labeled data about whether a direct marketing campaign from a bank would lead to a product being subscribed or not. The aim is to predict the label based on features such as age, job, marital status, education, loan etc.


**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**

## Scikit-learn Pipeline

**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

The pipeline consists of several steps including:

1. Data cleaning where:
* Features such as marital status, default and housing where binarized.
* categorical variables where one hot encoded and the original features removed.
* and string representations of months and days cast to integer representations.

2. As a classifier to do binary classification the logistic regression algorithm was used.
3. Two hyperparameters where tuned --C (regularization parameter to avoid overfitting, sampled from a uniform distribution) and --max_iter (number of iterations till sampled from an array of choices). 

The problem with the setup as is, is that the classes are imbalanced, the parameter tuning and 
automl solution both find an accuracy of 91% which is exactly fitting the majority class perfectly,hence accuracy
is a misleading metric. Because of the class imbalance, I introduced stratified sampling to the data preprocessing 
pipeline. 


**What are the benefits of the parameter sampler you chose?**

I have used random sampling, which is not a very smart way, but have first made a run with 
parameter ranges that where wider and then reduced the range of parameters to consider after the first run 
e.g. 32 is not converging during training, hence was ommitted. 

**What are the benefits of the early stopping policy you chose?**

The bandit policy enables runs to be terminated if they are not within a bandwith of the 
best performing metric so far. The lower the slack factor, the more runs will be terminated 
early which reduces computational cost.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**



## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

Logistic regression is one of the simplest models, it usually can only separate groups that are linearly separable
it is possible that the feature distributions are not linearly separable so another model was used. 



## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

In general it would be advisable to use more data and probably a metric that is insensitive to class imbalance
like F1 score or some balanced measure of precision and recall or AUC. 
However due to me balancing the sample already in this case this was alright. Better feature engineering is also definitely
a way to go for example polynomial features could help introduce nonlinearity to the linear model and improve
performance.

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
