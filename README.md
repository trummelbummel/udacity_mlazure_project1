# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparamters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)


## Summary

The dataset contains labeled data about whether a direct marketing campaign from a bank would lead to a product being subscribed or not. The aim is to predict the label based on features such as age, job, marital status, education, loan etc.

The best performing model was the AutoML derived Voting ensemble.

## Scikit-learn Pipeline

**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

The pipeline consists of several steps including:

1. Data cleaning where:
* Features such as marital status, default and housing where binarized.
* categorical variables where one hot encoded and the original features removed.
* and string representations of months and days cast to integer representations.

2. As a classifier to do binary classification the logistic regression algorithm was used.
3. Two hyperparameters where tuned --C (regularization parameter to avoid overfitting, sampled from a uniform distribution) and --max_iter (number of iterations till sampled from an array of choices). 

The problem with the setup as is, is that the classes are imbalanced, the parameter tuning and 
automl solution both find an accuracy of 91% which is exactly fitting the majority class perfectly,hence accuracy
is a misleading metric. Because of the class imbalance, I introduced stratified sampling to the data preprocessing 
pipeline. 


**What are the benefits of the parameter sampler you chose?**

I have used random sampling, which is not a very smart way, but have first made a run with 
parameter ranges that where wider and then reduced the range of parameters to consider after the first run 
e.g. 32 is not converging during training, hence was ommitted. Random sampling is usually efficient in finding
an optimum within 60 trials (see potential improvements.), plus as we do not employ grid search we 
search a more variable set of the parameter space to begin with.


**What are the benefits of the early stopping policy you chose?**

The bandit policy enables runs to be terminated if they are not within a bandwith of the 
best performing metric so far. The lower the slack factor, the more runs will be terminated 
early which reduces computational cost.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

Since I worked with a balanced dataset (stratified sampling) all models performance is around 89%. 
The best model was the voting ensemble, with duration and nr.employed being the most important
features. The ensemble consists of tree based methods and various versions of Scaling as well as
Truncated SVD as dimensionality reduction preprocessing step.


## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

Logistic regression is one of the simplest models and performs around 86% in hyperdrive and 88% in automl parametersearch on the balanced dataset.
The AutoML solution compares mostly tree based models which can create a non linear decision boundary opposed to the Logistic regression. In the AutoML solution there is no difference between the Logistic regression and the tree based methods, it follows that
the classes are likely separable with a linear decision boundary, and any differences are caused by the AutoML
Feature engineering steps.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

In general it would be advisable to use more data and probably a metric that is insensitive to class imbalance
like F1 score or some balanced measure of precision and recall or AUC. Future work could also explore more elaborate
ways of resampling the data e.g. SMOTE etc.
However due to me balancing the sample already in this case this was alright. Better feature engineering is also definitely
a way to go for example polynomial features could help introduce nonlinearity to the linear model and improve
performance. Furthermore we can also cast the problem as an anomaly detection problem as only 10% of the data is labeled as 
yes. Parametersearch should be done with more trials i.e. 60 as then in 95% we find optimal parameters.


## Proof of cluster clean up
In code.



